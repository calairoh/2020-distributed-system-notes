\subsection{Introduction}
In the distributed systems the natural distribution of the system complicates the synchronization, since we don't have a single local clock and/or a globally shared memory. For this reason there are some algorithms and protocols which aim to solve these problem, such as the synchronization between two or more machine clocks and the order of events in a distributed system.

\subsection{Physical clocks synchronization}
When we want to synchronize two or more physical clocks we have, basically, to face up with two requirements: \textbf{accuracy} and \textbf{agreement}. For \textit{accuracy} we mean that all clocks must be synchronized against a single one, with accurate time information, while, by the term \textit{agreement} we mean that all hosts must synchronize their clocks in the same way and with the same references between them.

When synchronizing clocks, one important thing is that temporal \textbf{monotony} must be preserved. In fact, if I discover to be ahead of time, typically I delay my timer, because it's dangerous go back with the timer because I can cause crashes.

\subsubsection{GPS}
One possible way to synchronize clocks is based on \textit{GPS (Global Position System)} mechanism. In other words, the idea is to exploit the \textit{GPS} procedure used to calculate the position of an object and, since the position is calculated by a triangulation procedure, that exploit the delays of signals to measure distance, it's possible to obtain also the perfect time.

In the practice, we need a fourth equation, and so a fourth satellite, to  obtain, as well as the three coordinates, the perfect time too. Obviously the procedure is  more complex since, in the previous description, we did some simplifications (\textit{e.g. the signal propagation speed is not constant}), but, however, this method is enough accurate, with a accuracy of few tens of nanoseconds.

\subsubsection{Cristian's algorithm}
The Cristian's algorithm was proposed by Flaviu Cristian in 1989 as a method for clock synchronization. The implementation is pretty simple and it's used, typically, in clock synchronization in distributed computer systems within small and low-latency intranets. The idea is to send a message to the server that will respond with its current time and trying to fix it adding the approximation of the time needed to send the message back from the server to the client.\\
Obviously, in order to fix the time received by the server we have to assume that the go and go back times are the same, or at least similar. In this way, the average, and so the temporal correction, will be pretty accurate.\\
\\
The time sets by the client will be:
\begin{equation*}
    T_c = T_s + T_{round} / 2
\end{equation*}
where $T_s$ is the time provided by the server, while $T_{round}$ is the time taken by the request for go to and from the server. If the previous assumption \textit{- go and go back times are the same -} doesn't hold, we commit an error. More precisely, we put the time in advance or in delay based on which message is faster.

Typically, the server to which request is sent is a machine that has some kind of info about the precise time, for example through a \textit{\textit{GPS procedure}}.

\subsubsection{Berkeley algorithm}
The Berkeley algorithm was developed at the University of California, Berkeley in 1989 as method for clock synchronization in distributed computing. Also here, the idea is pretty simple: a time server collects the time from all clients, average it, and then send to clients the required adjustment. As you may have noticed, the goal of this algorithm is not provide to the clients an absolute precise global clock, but only synchronize several machines in the net in order to share a common clock among them.
We can also optimize this algorithm with the Cristian's algorithm.

\subsubsection{Network Time Protocol (NTP)}
\textit{Network Time Protocol (NTP)} is a networking protocol for clock synchronization, used in practice over Internet, so used in nets with variable latency. The idea is to have a hierarchy organized in several layers, where the top level hosts are connected with an \textit{UTC} source and where the time information is propagated downward in the tree.

There are three main synchronization mechanisms:
\begin{itemize}
    \item \textit{Multicast (over LAN)} in which servers periodically multicast their time to other machines in the network
    \item \textit{Procedure-call mode} that is a procedure similar to Cristian's algorithm, in the sense that as well as sending the server clock, it estimates the offset and its accuracy.
    \item \textit{Symmetric mode} that is used for higher layers which need the highest accuracy
\end{itemize}

\subsection{Logical time}
In many practical cases, it's sufficient to agree on a time, even if it's not accurate w.r.t. the absolute time, in fact, in these cases, we are interested about the order, so if an event is occurred before or after someone else.\\
In order to better understand the logical time purpose, let's introduce the \textbf{happens-before} relationship.\\
Let's consider two distinct events, $e$ and $e'$. The two events are in \textbf{happens-before} relationship if at least one of these two conditions holds:
\begin{enumerate}
    \item If events $e$ and $e'$ occur in the same process and $e$ occurs before $e'$, then $e \rightarrow e'$
    \item If $e = send(msg)$ and $e' = recv(msg)$, then $e \rightarrow e'$
\end{enumerate}

\paragraph{Properties}
\begin{itemize}
    \item The relationship is \textbf{transitive}
    \item If neither $e \rightarrow e'$ nor $e' \rightarrow e$, the two events are concurrent $e || e'$
\end{itemize}

\subsubsection{Scalar clocks (Lamport clock)}
Leslie Lamport created a procedure in which we can capture the \textit{happend-before} relationship numerically by using integers to represent the clock value and have no relationship with a physical clock whatsoever.

\paragraph{Implementation}
\begin{itemize}
    \item \textit{Initial condition}: every process has an integer counter initialized at 0, that we will call $L_i$
    \item Every time a message is sent by process $i$, this is timestamped with $L_i$
    \item $L_i$ is incremented:
    \begin{enumerate}
        \item Before process $i$ sends a message
        \item Upon receipt of a message, process $i$ sets $L_i$ to $\texttt{MAX}(ts(msg), L_i) + 1$, where $ts(msg)$ is the timestamp contained in the message received.
    \end{enumerate}
\end{itemize}

\begin{figure}[h]
    \caption{An example of Lamport clock implementation}
    \includegraphics[scale=0.5]{src/images/synchronization/scalar-clock.png}
    \centering
\end{figure}

\textbf{Note that}, this procedure give us only a partial order. In order to achieve a total order we can simply attach the process ID to the integer counter, so, for example, counter $1.3$ tells us that the current counter value is $1$ in process $3$. It's important to notice that achieve a total order doesn't mean there are no parallel events.

\subsubsection{Vector clocks}
In scalar clocks, from the \textit{happens-before} relationship we know that $e \rightarrow e' \implies L(e) < L(e')$ but, as we can see, this is an implication, not a double implication, so the reverse doesn't necessary hold. In order to solve this problem there were introduced the vector clocks.

\paragraph{Implementation}

In vector clocks each process maintains a vector of $N$ values (where $N$ is the number of processes), such that every process has a snapshot of the situation of the other processes.\\
So, let's define the vector $V_i[N]$ as the vector of the process $i$. Each process $i$ knows something about the situation of other processes, so if we have $V_i[j] = k$, this means that process $i$ knows that $k$ events occurred in process $j$.

\begin{itemize}
    \item \textit{Initial condition}: every process has a vector where all cells are initialized at 0, so $V_i[j] = 0 \forall i, j$
    \item Every time a message is sent by process $i$, this is timestamped with $L_i$
    \item Increments:
    \begin{enumerate}
        \item $V_i[i]$ is incremented just before process $i$ sends a message
        \item Upon receipt of a message, process $i$ sets $V_i[j] = \texttt{MAX}(ts(msg), V_i[j]) \texttt{  } \forall i \neq j$, where $ts(msg)$ is the timestamp contained in the message received, and then increments $V_i[i]$.
    \end{enumerate}
\end{itemize}

\begin{figure}[h]
    \caption{An example of vector clock implementation}
    \includegraphics[scale=0.5]{src/images/synchronization/vector-clock.png}
    \centering
\end{figure}

\paragraph{Causal delivery}
We can also use vector clocks, with slight variations, in order to implement causal delivery of messages in a totally distributed way. In practice, we increment the clock only when sending a message and, on receive, we just merge, without any increment.

\begin{figure}[h]
    \caption{An example of vector clock implementation}
    \includegraphics[scale=0.5]{src/images/synchronization/vector-clock-causality.png}
    \centering
\end{figure}

\subsection{Mutual exclusion}
The \textbf{mutual exclusion} aims to prevent interference and ensure consistency of resource access among several processes. In order to face up with this problem we assume to have \textbf{reliable channels and processes}. Moreover, the mutual exclusion has to satisfy two main requirements: \textbf{safety}, which means than only one process may execute in the critical section at a time, and \textbf{liveness}, which means that all requests to enter/exit the critial section eventually succeed.

\subsubsection{Mutual exclusion with scalar clocks}
The procedure is pretty simple and it exploits the scalar clock methodology with the goal to use timestamps as a metric for resource assignment.

In details, a process multicasts a resource request message to all processes, including its timestamp. When processes receive the message, they have three options:
\begin{enumerate}
    \item If it doesn't hold the resource and it's not interested to hold it, simply replies with an \textit{ACK}
    \item If it holds the resource, puts the requests into a local queue ordered according to timestamps
    \item If it doesn't hold the resource, it's interested to hold it but it has already sent a request message, it replies with an \textit{ACK} if the incoming request message has a lower timestamp, otherwise put it on a local queue
\end{enumerate}
When releasing a resource, a process acknowledges all the requests queued and the resource is granted to a process when its request has been acknowledged by all processes.

\subsubsection{Token ring solution}
Another possible solution used to provide mutual exclusion is the \textbf{token ring}. In order to apply this mechanism, the processes must be logically arranged in a ring (don't care if their physical connections are not arranged in a ring).
The idea is that only the owner of the token can access a certain resource. The token is forwarded from process to process through the ring network. When the token is received by a process $P$ can perform three different behaviours:
\begin{itemize}
    \item If $P$ is not interested in accessing the resource, it forwards the token
    \item If $P$ is interested in accessing the resource, $P$ retains the token and accesses the resource.
\end{itemize}
When operations on the resource are finished, this one is released and the token forwarded to the next process.

\subsection{Leader election}
We know that many distributed algorithms and procedures need a coordinator, but in this case we have the agreement problem, that is, all processes in the systems must agree on a new leader. 

In order to do it, we have to do some assumptions. First of all \textbf{nodes must be distinguishable} and, moreover, the \textbf{system must be closed}, in the sense that processed know each other and their IDs.


\subsubsection{Bully election algorithm}
The idea of this algorithm is that, once detected the coordinator failure, a new election starts. If the algorithm runs right, at the end, the process with the highest ID will be elected as coordinator, hence the name of the algorithm. In order to make this algorithm works right, we will have to do some additional assumptions: \textbf{links are reliable} and \textbf{it's possible to detect the failed process}.

\paragraph{Procedure}
\begin{enumerate}
    \item Process $P$ detects that the coordinator is no more available and so it starts a new election by sending an \textit{ELECT} message, including its ID, to all other processes with higher IDs.
    \item When a process $P'$ receives an \textit{ELECT} message it responds to the sender and starts a new election, if it has not already started.
    \item If no one responds to a process $P''$ that has sent an \textit{ELECT} message, $P''$ sends a \textit{COORD} message to all other processes in the net with a lower ID.
\end{enumerate}

It's important to notice that if the previous coordinator process can hold a new election when it comes back up, and so take back the role of coordinator, since we assume that, having been a coordinator, has a higher ID than the current coordinator.

\subsubsection{Ring-based algorithm}
This algorithm is based on the assumption that nodes (processes) are arranged in a physical or logical ring network. The procedure is quite similar w.r.t. the \textit{bully election algorithm}.

\paragraph{Procedure}
\begin{enumerate}
    \item Process $P$ detects that the coordinator is no more available and so it starts a new election by sending an \textit{ELECT} message, including its ID, to the next alive neighbor.
    \item When a process $P'$ receives an \textit{ELECT} message it checks if its ID is in the list. If so, it change the message type to \textit{COORD}, otherwise it adds its own ID to the list. In both cases the new message is forwarded to the next alive neighbor. 
    \item When a process $P'$ receives a \textit{COORD} message it takes the highest ID in the list and set it as coordinator.
\end{enumerate}

\textbf{Note that}, when the \textit{COORD} message is received, the host is also informed about the remaining members of the ring and if multiple processes starts a new election procedure, the result will converge to the same content and so to the same (new) coordinator.

\subsection{Collecting global state}
Since we have a distributed system, a global state consists of the local state of each process and the messages in transit over the links. It's important to notice that collecting a global state is easier if we have a global clock, but in distributed systems we don't have it, so, potentially, we have to record the process states in different times and merge the information in such a way that we have a consistent global state.

\subsubsection{Distributed snapshot}
A \textbf{distributed snapshot} reflects a state in which the distributed system might have been and obviously this snapshot must be \textit{consistent}. Moreover, we introduce also the \textit{cut} definition. A \textbf{cut} is the union of the histories of all processes in a system up to a certain event. Note that, a cut defined as before can also be not consistent \textit{(for example, look at the figure 23)}. So, we define also the \textit{consistent cut}. A \textbf{consistent cut} is a cut in which for every event \textit{e} included, the cut includes also all the events that happened before \textit{e}. In other words, when a message received on process \textit{P'} is recorded, the message sending on process \textit{P} must be recorded as well, but the contrary is not required. 

\begin{figure}[h]
    \caption{An example of consistent and not consistent cut}
    \includegraphics[scale=0.5]{src/images/synchronization/consistent-cut.png}
    \centering
\end{figure}

\subsubsection{Chandy-Lamport algrithm}
\textit{Chandy-Lamport algorithm} is a snapshot algorithm that is used in distributed systems for recording a consistent global state of an asynchronous system. In order to perform this algorithm we assume we have reliable links and nodes (processes) and a FIFO policy concerning the queue transit method. 

\paragraph{Procedure}
A process \textit{P} runs the algorithm when a specific token is received or, in alternative, when process \textit{P} itself decides to start the procedure.
\begin{itemize}
    \item The observer process (the process taking a snapshot):
    \begin{enumerate}
        \item Saves its own local state
        \item Sends a snapshot request message bearing a snapshot token to all other processes
    \end{enumerate}
    \item A process receiving the snapshot token for the first time on any message:
    \begin{enumerate}
        \item Sends the observer process its own saved state
        \item Attaches the snapshot token to all subsequent messages (to help propagate the snapshot token)
    \end{enumerate}
    \item When a process that has already received the snapshot token receives a message that does not bear the snapshot token, this process will forward that message to the observer process
\end{itemize}

\textbf{Note that,} it's not required to block the computation, in fact, the operation of collecting the snapshots is interleaved with processing. Moreover, we can start the \textit{distributed snapshot} in different locations at the same time, since we can associate a specific identifier to each snapshot in order to identify them.

\begin{figure}[h]
    \caption{An example of a Chandy-Lamport algorithm execution}
    \includegraphics[scale=0.4]{src/images/synchronization/distributed-snapshot.png}
    \centering
\end{figure}

\paragraph{Theorem}
The distributed snapshot algorithm selects a consistent cut.\\
\textbf{Proof}\\
Let $e_i$ and $e_j$ be two events occurring at process $p_i$ and $p_j$, respectively, such that $e_i \to e_j$.\\
Suppose $e_j$ us part of the cut and $e_i$ is not. This means $e_j$ occurred before $p_j$ records its state, while $e_i$ occurred after $p_i$ saved its state.\\
If $p_i = p_j$ this is trivially impossible.\\
If $p_i \neq p_j$, let's consider $m_1, ..., m_h$ be the sequence of messages that give rise to the relation $e_i \to e_j$.\\
If $e_i$ occurred after $p_i$ saved its state then $p_i$ sent a marker ahead of $m_1, ..., m_h$. By FIFO ordering over channels and by the marker propagating rules it results that $p_j$ received a marker ahead of $m_1,...,m_h$. By the marker processing rule it results that $p_j$ saved its state before receiving $m_1,...,m_h$, i.e., before $e_j$ , which contradicts our initial assumption.